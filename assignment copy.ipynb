{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are expected to use this support code.\n",
    "# You may want to write:\n",
    "# from ct_support_code import *\n",
    "# at the top of your answers\n",
    "\n",
    "# You will need NumPy and SciPy:\n",
    "from ct_support_code import *\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import cho_factor, cho_solve\n",
    "\n",
    "\n",
    "def params_unwrap(param_vec, shapes, sizes):\n",
    "    \"\"\"Helper routine for minimize_list\"\"\"\n",
    "    args = []\n",
    "    pos = 0\n",
    "    for i in range(len(shapes)):\n",
    "        sz = sizes[i]\n",
    "        args.append(param_vec[pos:pos+sz].reshape(shapes[i]))\n",
    "        pos += sz\n",
    "    return args\n",
    "\n",
    "\n",
    "def params_wrap(param_list):\n",
    "    \"\"\"Helper routine for minimize_list\"\"\"\n",
    "    param_list = [np.array(x) for x in param_list]\n",
    "    shapes = [x.shape for x in param_list]\n",
    "    sizes = [x.size for x in param_list]\n",
    "    param_vec = np.zeros(sum(sizes))\n",
    "    pos = 0\n",
    "    for param in param_list:\n",
    "        sz = param.size\n",
    "        param_vec[pos:pos+sz] = param.ravel()\n",
    "        pos += sz\n",
    "    unwrap = lambda pvec: params_unwrap(pvec, shapes, sizes)\n",
    "    return param_vec, unwrap\n",
    "\n",
    "\n",
    "def minimize_list(cost, init_list, args):\n",
    "    \"\"\"Optimize a list of arrays (wrapper of scipy.optimize.minimize)\n",
    "\n",
    "    The input function \"cost\" should take a list of parameters,\n",
    "    followed by any extra arguments:\n",
    "        cost(init_list, *args)\n",
    "    should return the cost of the initial condition, and a list in the same\n",
    "    format as init_list giving gradients of the cost wrt the parameters.\n",
    "\n",
    "    The options to the optimizer have been hard-coded. You may wish\n",
    "    to change disp to True to get more diagnostics. You may want to\n",
    "    decrease maxiter while debugging. Although please report all results\n",
    "    in Q2-5 using maxiter=500.\n",
    "    \"\"\"\n",
    "    opt = {'maxiter': 500, 'disp': False}\n",
    "    init, unwrap = params_wrap(init_list)\n",
    "    def wrap_cost(vec, *args):\n",
    "        E, params_bar = cost(unwrap(vec), *args)\n",
    "        vec_bar, _ = params_wrap(params_bar)\n",
    "        return E, vec_bar\n",
    "    res = minimize(wrap_cost, init, args, 'L-BFGS-B', jac=True, options=opt)\n",
    "    return unwrap(res.x)\n",
    "\n",
    "\n",
    "def linreg_cost(params, X, yy, alpha):\n",
    "    \"\"\"Regularized least squares cost function and gradients\n",
    "\n",
    "    Can be optimized with minimize_list -- see fit_linreg_gradopt for a\n",
    "    demonstration.\n",
    "\n",
    "    Inputs:\n",
    "    params: tuple (ww, bb): weights ww (D,), bias bb scalar\n",
    "         X: N,D design matrix of input features\n",
    "        yy: N,  real-valued targets\n",
    "     alpha: regularization constant\n",
    "\n",
    "    Outputs: (E, [ww_bar, bb_bar]), cost and gradients\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb = params\n",
    "\n",
    "    # forward computation of error\n",
    "    ff = np.dot(X, ww) + bb\n",
    "    res = ff - yy\n",
    "    E = np.dot(res, res) + alpha*np.dot(ww, ww)\n",
    "\n",
    "    # reverse computation of gradients\n",
    "    ff_bar = 2*res\n",
    "    bb_bar = np.sum(ff_bar)\n",
    "    ww_bar = np.dot(X.T, ff_bar) + 2*alpha*ww\n",
    "\n",
    "    return E, [ww_bar, bb_bar]\n",
    "\n",
    "\n",
    "def fit_linreg_gradopt(X, yy, alpha):\n",
    "    \"\"\"\n",
    "    fit a regularized linear regression model with gradient opt\n",
    "\n",
    "         ww, bb = fit_linreg_gradopt(X, yy, alpha)\n",
    "\n",
    "     Find weights and bias by using a gradient-based optimizer\n",
    "     (minimize_list) to improve the regularized least squares cost:\n",
    "\n",
    "       np.sum(((np.dot(X,ww) + bb) - yy)**2) + alpha*np.dot(ww,ww)\n",
    "\n",
    "     Inputs:\n",
    "             X N,D design matrix of input features\n",
    "            yy N,  real-valued targets\n",
    "         alpha     scalar regularization constant\n",
    "\n",
    "     Outputs:\n",
    "            ww D,  fitted weights\n",
    "            bb     scalar fitted bias\n",
    "    \"\"\"\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    init = (np.zeros(D), np.array(0))\n",
    "    ww, bb = minimize_list(linreg_cost, init, args)\n",
    "    return ww, bb\n",
    "\n",
    "\n",
    "def logreg_cost(params, X, yy, alpha):\n",
    "    \"\"\"Regularized logistic regression cost function and gradients\n",
    "\n",
    "    Can be optimized with minimize_list -- see fit_linreg_gradopt for a\n",
    "    demonstration of fitting a similar function.\n",
    "\n",
    "    Inputs:\n",
    "    params: tuple (ww, bb): weights ww (D,), bias bb scalar\n",
    "         X: N,D design matrix of input features\n",
    "        yy: N,  real-valued targets\n",
    "     alpha: regularization constant\n",
    "\n",
    "    Outputs: (E, [ww_bar, bb_bar]), cost and gradients\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb = params\n",
    "\n",
    "    # Force targets to be +/- 1\n",
    "    yy = 2*(yy==1) - 1\n",
    "\n",
    "    # forward computation of error\n",
    "    aa = yy*(np.dot(X, ww) + bb)\n",
    "    sigma = 1/(1 + np.exp(-aa))\n",
    "    E = -np.sum(np.log(sigma)) + alpha*np.dot(ww, ww)\n",
    "\n",
    "    # reverse computation of gradients\n",
    "    aa_bar = sigma - 1\n",
    "    bb_bar = np.dot(aa_bar, yy)\n",
    "    ww_bar = np.dot(X.T, yy*aa_bar) + 2*alpha*ww\n",
    "\n",
    "    return E, (ww_bar, bb_bar)\n",
    "\n",
    "\n",
    "def nn_cost(params, X, yy=None, alpha=None):\n",
    "    \"\"\"NN_COST simple neural network cost function and gradients, or predictions\n",
    "\n",
    "           E, params_bar = nn_cost([ww, bb, V, bk], X, yy, alpha)\n",
    "                    pred = nn_cost([ww, bb, V, bk], X)\n",
    "\n",
    "     Cost function E can be minimized with minimize_list\n",
    "\n",
    "     Inputs:\n",
    "             params (ww, bb, V, bk), where:\n",
    "                    --------------------------------\n",
    "                        ww K,  hidden-output weights\n",
    "                        bb     scalar output bias\n",
    "                         V K,D hidden-input weights\n",
    "                        bk K,  hidden biases\n",
    "                    --------------------------------\n",
    "                  X N,D input design matrix\n",
    "                 yy N,  regression targets\n",
    "              alpha     scalar regularization for weights\n",
    "\n",
    "     Outputs:\n",
    "                     E  sum of squares error\n",
    "            params_bar  gradients wrt params, same format as params\n",
    "     OR\n",
    "               pred N,  predictions if only params and X are given as inputs\n",
    "    \"\"\"\n",
    "    # Unpack parameters from list\n",
    "    ww, bb, V, bk = params\n",
    "\n",
    "    # Forwards computation of cost\n",
    "    A = np.dot(X, V.T) + bk[None,:] # N,K\n",
    "    P = 1 / (1 + np.exp(-A)) # N,K\n",
    "    F = np.dot(P, ww) + bb # N,\n",
    "    if yy is None:\n",
    "        # user wants prediction rather than training signal:\n",
    "        return F\n",
    "    res = F - yy # N,\n",
    "    E = np.dot(res, res) + alpha*(np.sum(V*V) + np.dot(ww,ww)) # 1x1\n",
    "\n",
    "    # Reverse computation of gradients\n",
    "    F_bar = 2*res # N,\n",
    "    ww_bar = np.dot(P.T, F_bar) + 2*alpha*ww # K,\n",
    "    bb_bar = np.sum(F_bar) # scalar\n",
    "    P_bar = np.dot(F_bar[:,None], ww[None,:]) # N,K\n",
    "    A_bar = P_bar * P * (1 - P) # N,K\n",
    "    V_bar = np.dot(A_bar.T, X) + 2*alpha*V # K,D\n",
    "    bk_bar = np.sum(A_bar, 0)\n",
    "\n",
    "    return E, (ww_bar, bb_bar, V_bar, bk_bar)\n",
    "\n",
    "\n",
    "def rbf_fn(X1, X2):\n",
    "    \"\"\"Helper routine for gp_post_par\"\"\"\n",
    "    return np.exp((np.dot(X1,(2*X2.T))-np.sum(X1*X1,1)[:,None]) - np.sum(X2*X2,1)[None,:])\n",
    "\n",
    "\n",
    "def gauss_kernel_fn(X1, X2, ell, sigma_f):\n",
    "    \"\"\"Helper routine for gp_post_par\"\"\"\n",
    "    return sigma_f**2 * rbf_fn(X1/(np.sqrt(2)*ell), X2/(np.sqrt(2)*ell))\n",
    "\n",
    "\n",
    "def gp_post_par(X_rest, X_obs, yy, sigma_y=0.05, ell=5.0, sigma_f=0.1):\n",
    "    \"\"\"GP_POST_PAR means and covariances of a posterior Gaussian process\n",
    "\n",
    "         rest_cond_mu, rest_cond_cov = gp_post_par(X_rest, X_obs, yy)\n",
    "         rest_cond_mu, rest_cond_cov = gp_post_par(X_rest, X_obs, yy, sigma_y, ell, sigma_f)\n",
    "\n",
    "     Calculate the means and covariances at all test locations of the posterior Gaussian\n",
    "     process conditioned on the observations yy at observed locations X_obs.\n",
    "\n",
    "     Inputs:\n",
    "                 X_rest GP test locations\n",
    "                  X_obs locations of observations\n",
    "                     yy observed values\n",
    "                sigma_y observation noise standard deviation\n",
    "                    ell kernel function length scale\n",
    "                sigma_f kernel function standard deviation\n",
    "\n",
    "     Outputs:\n",
    "           rest_cond_mu mean at each location in X_rest\n",
    "          rest_cond_cov covariance matrix between function values at all test locations\n",
    "    \"\"\"\n",
    "    X_rest = X_rest[:, None]\n",
    "    X_obs = X_obs[:, None]\n",
    "    K_rest = gauss_kernel_fn(X_rest, X_rest, ell, sigma_f)\n",
    "    K_rest_obs = gauss_kernel_fn(X_rest, X_obs, ell, sigma_f)\n",
    "    K_obs = gauss_kernel_fn(X_obs, X_obs, ell, sigma_f)\n",
    "    M = K_obs + sigma_y**2 * np.eye(yy.size)\n",
    "    M_cho, M_low = cho_factor(M)\n",
    "    rest_cond_mu = np.dot(K_rest_obs, cho_solve((M_cho, M_low), yy))\n",
    "    rest_cond_cov = K_rest - np.dot(K_rest_obs, cho_solve((M_cho, M_low), K_rest_obs.T))\n",
    "\n",
    "    return rest_cond_mu, rest_cond_cov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ct_support_code import *\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import cho_factor, cho_solve\n",
    "\n",
    "# Load the data. \n",
    "data = np.load('data/ct_data.npz')\n",
    "\n",
    "# Train, validation, and test sets... Do not shuffle the data further. \n",
    "X_train = data['X_train']; X_val = data['X_val']; X_test = data['X_test']\n",
    "y_train = data['y_train']; y_val = data['y_val']; y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (40754, 384), y_train: (40754,), X_val: (5785, 384), y_val: (5785,), X_test: (6961, 384), y_test: (6961,)\n"
     ]
    }
   ],
   "source": [
    "# The input feature vectors are 384 dimensional. The target variable is cts. \n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}, X_val: {X_val.shape}, y_val: {y_val.shape}, X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "       -0.25    , -0.25    , -0.25    , -0.25    ,  0.      ,  0.      ,\n",
       "        0.      ,  0.8     ,  0.      ,  0.      ,  0.      , -0.25    ,\n",
       "       -0.25    , -0.25    ,  0.      ,  0.      ,  0.      ,  0.956533,\n",
       "        0.8521  ,  0.      ,  0.      ,  0.      ,  0.      , -0.25    ,\n",
       "        0.      ,  0.      ,  0.      ,  0.932476,  0.960794,  0.      ,\n",
       "        0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "        0.      ,  0.791258,  0.979383,  0.8725  ,  0.      ,  0.      ,\n",
       "        0.      , -0.25    ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "        0.16    ,  0.979998,  0.954111,  0.      , -0.25    , -0.25    ,\n",
       "        0.      ,  0.      ,  0.      ,  0.      ,  0.689167,  0.987036,\n",
       "        0.92411 ,  0.      , -0.25    , -0.25    ,  0.      ,  0.      ,\n",
       "        0.      ,  0.867052,  0.975287,  0.813456,  0.      ,  0.      ,\n",
       "        0.      , -0.25    ,  0.      ,  0.      ,  0.      ,  0.934405,\n",
       "        0.903144,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "        0.      ,  0.      ,  0.      ,  0.977522,  0.216   ,  0.      ,\n",
       "        0.      ,  0.      ,  0.      , -0.25    ,  0.      ,  0.      ,\n",
       "        0.      ,  0.      ,  0.      ,  0.      ,  0.      , -0.25    ,\n",
       "       -0.25    , -0.25    ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "        0.      ,  0.      , -0.25    , -0.25    , -0.25    , -0.25    ,\n",
       "        0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "       -0.25    , -0.25    , -0.25    , -0.25    ,  0.      ,  0.      ,\n",
       "        0.      ,  0.      ,  0.      ,  0.      ,  0.      , -0.25    ,\n",
       "       -0.25    , -0.25    ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "        0.      ,  0.      ,  0.      ,  0.      ,  0.      , -0.25    ,\n",
       "        0.615385,  0.81448 ,  0.98211 ,  0.236967,  0.      ,  0.      ,\n",
       "        0.      ,  0.      ,  0.      ,  0.      ,  0.825   ,  0.868957,\n",
       "        0.912158,  0.494234,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "        0.      , -0.25    ,  0.802469,  0.529412,  0.      ,  0.939733,\n",
       "        0.856388,  0.      ,  0.      ,  0.      , -0.25    , -0.25    ,\n",
       "        0.857595,  0.853659,  0.589474,  0.558042,  0.      ,  0.      ,\n",
       "        0.      ,  0.      , -0.25    , -0.25    ,  0.910979,  0.776817,\n",
       "        0.938913,  0.584594,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "        0.      , -0.25    ,  0.      ,  0.137755,  0.829091,  0.394059,\n",
       "        0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "        0.      ,  0.609023,  0.686546,  0.      ,  0.      ,  0.      ,\n",
       "        0.      ,  0.      ,  0.      , -0.25    ,  0.      ,  0.      ,\n",
       "        0.      ,  0.      ,  0.      ,  0.      ,  0.      , -0.25    ,\n",
       "       -0.25    , -0.25    ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "        0.      ,  0.      , -0.25    , -0.25    , -0.25    , -0.25    ,\n",
       "        0.997728,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "       -0.25    , -0.25    ,  0.997828,  0.      ,  0.      ,  0.      ,\n",
       "        0.      ,  0.      ,  0.      , -0.25    ,  0.993141,  0.      ,\n",
       "        0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "        0.991082,  0.      ,  0.      ,  0.      ,  0.973954,  0.      ,\n",
       "        0.      ,  0.      ,  0.960748,  0.      ,  0.      ,  0.712107,\n",
       "        0.997945,  0.      ,  0.      , -0.25    ,  0.852405,  0.      ,\n",
       "        0.      ,  0.955319,  0.953333,  0.      ,  0.      ,  0.      ,\n",
       "        0.920322,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "        0.      ,  0.      ,  0.997512,  0.      ,  0.      ,  0.      ,\n",
       "        0.      ,  0.      ,  0.      , -0.25    ,  0.999652,  0.665803,\n",
       "        0.      ,  0.      ,  0.      ,  0.      , -0.25    , -0.25    ,\n",
       "        0.997679,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "       -0.25    , -0.25    ,  0.992384,  0.      ,  0.      ,  0.      ,\n",
       "        0.      ,  0.      ,  0.      , -0.25    ,  0.99489 ,  0.      ,\n",
       "        0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "        0.992278,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "        0.      ,  0.      ,  0.99194 ,  0.      ,  0.      ,  0.      ,\n",
       "        0.      ,  0.      ,  0.      , -0.25    ,  0.967973,  0.      ,\n",
       "        0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "        0.850746,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "        0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "        0.      ,  0.      ,  0.      , -0.25    ,  0.980381,  0.      ,\n",
       "        0.      ,  0.      ,  0.      ,  0.      , -0.25    , -0.25    ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0] # The input feature vectors are normalized between 0 and 1. Additionally, they're quite sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 2.2265180851800834, Min: -1.8679386519531087, Mean: -9.13868774539957e-15, Std Dev: 0.9999877311903766\n"
     ]
    }
   ],
   "source": [
    "# Max, min, average value of y_train, st dev of y_train\n",
    "print(f\"Max: {np.max(y_train)}, Min: {np.min(y_train)}, Mean: {np.mean(y_train)}, Std Dev: {np.std(y_train)}\")\n",
    "# The target variable follows a standard normal distribution with mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7617570093457944, Validation: 0.10813084112149533, Test: 0.13011214953271028\n"
     ]
    }
   ],
   "source": [
    "# What is the train/validation/test split? in terms of percentages. \n",
    "print(f\"Train: {X_train.shape[0]/(X_train.shape[0] + X_val.shape[0] + X_test.shape[0])}, Validation: {X_val.shape[0]/(X_train.shape[0] + X_val.shape[0] + X_test.shape[0])}, Test: {X_test.shape[0]/(X_train.shape[0] + X_val.shape[0] + X_test.shape[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 384 features extracted from CT images. The class variable is numeric and denotes the relative location of the CT slice on the axial axis of the human body.\n",
    "\n",
    "* What do the features represent?\n",
    "    * 1 - 240.:         Histogram describing bone structures\n",
    "    * 241 - 384.:       Histogram describing air inclusions\n",
    "\n",
    "Each CT slice is described by two histograms in polar space.\n",
    "The first histogram describes the location of bone structures in the image,\n",
    "the second the location of air inclusions inside of the body.\n",
    "Both histograms are concatenated to form the final feature vector.\n",
    "Bins that are outside of the image are marked with the value -0.25.\n",
    "\n",
    "The class variable (relative location of an image on the axial axis) was\n",
    "constructed by manually annotating up to 10 different distinct landmarks in\n",
    "each CT Volume with known location. The location of slices in between\n",
    "landmarks was interpolated.\n",
    "\n",
    "The data was retrieved from a set of 53500 CT images from 74 different\n",
    "patients (43 male, 31 female). The first 73 patients were put in the _train arrays, the next 12 in the _val arrays, and the final 12 in the _test arrays. Please use this training, validation, test split as given. Do not shuffle the data further in this assignment.\n",
    "\n",
    "### Note: Your answers should be executable and should PRINT requested information. Also, manually include as part of worded answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.13868774539957e-15\n",
      "-0.2160085093241599\n"
     ]
    }
   ],
   "source": [
    "# Verify that (up to numerical rounding errors) the mean of the training positions in y_train is zero. \n",
    "sample_mean_y_train = np.mean(y_train)\n",
    "print(sample_mean_y_train)\n",
    "\n",
    "# The mean of the 5,785 positions in the y_val array is not zero.\n",
    "sample_mean_y_val = np.mean(y_val)\n",
    "print(sample_mean_y_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data into Python (your code can assume this step has been done):\n",
    "\n",
    "* Verify that (up to numerical rounding errors) the mean of the training positions in y_train is zero. The mean of the 5,785 positions in the y_val array is not zero. Report its mean with a “standard error”, temporarily assuming that each entry is independent. For comparison, also report the mean with a standard error of the first 5,785 entries in the y_train. Explain how your results demonstrate that these standard error bars do not reliably indicate what the average of locations in future CT slice data will be. Why are standard error bars misleading here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample mean of y_train: -9.13868774539957e-15\n",
      "Sample mean of y_val: -0.2160085093241599\n",
      "Sample mean of first 5785 positions in y_train: -0.44247687859693674\n",
      "Sample std dev of first 5785 positions in y_train: 0.9071810045985477\n",
      "Standard error of first 5785 positions in y_train: 0.011927303389170828\n",
      "Sample mean of y_val: -0.2160085093241599\n",
      "Sample std dev of y_val: 0.9815056935674723\n",
      "Standard error of y_val: 0.01290449880016868\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Verify that (up to numerical rounding errors) the mean of the training positions in y_train is zero.\n",
    "sample_mean_y_train = np.mean(y_train)\n",
    "print(f\"Sample mean of y_train: {sample_mean_y_train}\")\n",
    "\n",
    "# The mean of the 5,785 positions in the y_val array is not zero.\n",
    "sample_mean_y_val = np.mean(y_val)\n",
    "print(f\"Sample mean of y_val: {sample_mean_y_val}\")\n",
    "\n",
    "# Calculate the sample mean and standard error of the first 5785 positions in y_train\n",
    "truncated_sample_mean_y_train = np.mean(y_train[:5785])\n",
    "truncated_sample_std_y_train = np.std(y_train[:5785], ddof=1)  # using ddof=1 for sample std deviation\n",
    "N = 5785\n",
    "truncated_st_error_y_train = truncated_sample_std_y_train / np.sqrt(N)\n",
    "print(f\"Sample mean of first 5785 positions in y_train: {truncated_sample_mean_y_train}\")\n",
    "print(f\"Sample std dev of first 5785 positions in y_train: {truncated_sample_std_y_train}\")\n",
    "print(f\"Standard error of first 5785 positions in y_train: {truncated_st_error_y_train}\")\n",
    "\n",
    "# Calculate the sample standard deviation and standard error of y_val\n",
    "sample_std_y_val = np.std(y_val, ddof=1)  # using ddof=1 for sample std deviation\n",
    "std_error_y_val = sample_std_y_val / np.sqrt(N)\n",
    "print(f\"Sample mean of y_val: {sample_mean_y_val}\")\n",
    "print(f\"Sample std dev of y_val: {sample_std_y_val}\")\n",
    "print(f\"Standard error of y_val: {std_error_y_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the input features are constants: they take on the same value for every training example. Identify these features, and remove them from the input matrices in the training, validation, and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed constant feature indices: [ 59  69 179 189 351]\n",
      "Removed duplicate feature indices: [76, 77, 185, 195, 283, 354]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to remove constant features\n",
    "def remove_constant_features(X_train, X_val, X_test):\n",
    "    \n",
    "    # Identify constant columns\n",
    "    constant_mask = np.all(X_train == X_train[0, :], axis=0)  # True for constant columns\n",
    "    constant_indices = np.where(constant_mask)[0]  # Indices of constant columns\n",
    "    \n",
    "    # Remove these columns from all datasets\n",
    "    X_train = X_train[:, ~constant_mask]\n",
    "    X_val = X_val[:, ~constant_mask]\n",
    "    X_test = X_test[:, ~constant_mask]\n",
    "    \n",
    "    return X_train, X_val, X_test, constant_indices\n",
    "\n",
    "def remove_duplicate_features(X_train, X_val, X_test):\n",
    "    # Initialize a list to store indices of unique columns\n",
    "    unique_indices = []\n",
    "    duplicate_indices = []\n",
    "    seen_columns = {}\n",
    "    \n",
    "    for idx in range(X_train.shape[1]):\n",
    "        # Convert column to a tuple (hashable type) for dictionary storage\n",
    "        col_tuple = tuple(X_train[:, idx])\n",
    "        if col_tuple not in seen_columns:\n",
    "            seen_columns[col_tuple] = idx\n",
    "            unique_indices.append(idx)\n",
    "        else:\n",
    "            duplicate_indices.append(idx)\n",
    "    \n",
    "    # Keep only unique columns in the original order\n",
    "    X_train = X_train[:, unique_indices]\n",
    "    X_val = X_val[:, unique_indices]\n",
    "    X_test = X_test[:, unique_indices]\n",
    "    \n",
    "    return X_train, X_val, X_test, duplicate_indices\n",
    "\n",
    "\n",
    "# Perform the cleaning steps\n",
    "X_train, X_val, X_test, constant_indices = remove_constant_features(X_train, X_val, X_test)\n",
    "X_train, X_val, X_test, duplicate_indices = remove_duplicate_features(X_train, X_val, X_test)\n",
    "\n",
    "# Report removed columns\n",
    "print(\"Removed constant feature indices:\", constant_indices)\n",
    "print(\"Removed duplicate feature indices:\", duplicate_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shapes: X_train: (40754, 373), X_val: (5785, 373), X_test: (6961, 373)\n"
     ]
    }
   ],
   "source": [
    "print(f\"New shapes: X_train: {X_train.shape}, X_val: {X_val.shape}, X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on training set (closed-form): 0.3567565397204054\n",
      "RMSE on training set (gradient-based): 0.35675704441316\n",
      "RMSE on validation set (closed-form): 0.4230521968394692\n",
      "RMSE on validation set (gradient-based): 0.4230514057395294\n",
      "Weight difference: 0.003069446594797705\n",
      "Bias difference: 0.0007478002387096144\n",
      "Are the weights the same? True\n",
      "Are the biases the same? True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ct_support_code import *\n",
    "\n",
    "def fit_linreg(X, yy, alpha):\n",
    "    # Add a column of ones to X to account for the bias term\n",
    "    X_augmented = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "    \n",
    "    # Augment X and yy for regularization\n",
    "    reg_matrix = np.sqrt(alpha) * np.eye(X.shape[1] + 1)\n",
    "    reg_matrix[-1, -1] = 0  # Do not regularize the bias term\n",
    "    \n",
    "    X_reg = np.vstack([X_augmented, reg_matrix])\n",
    "    yy_reg = np.concatenate([yy, np.zeros(X.shape[1] + 1)])\n",
    "    \n",
    "    # Solve using lstsq to obtain the weights and bias\n",
    "    params, _, _, _ = np.linalg.lstsq(X_reg, yy_reg, rcond=None)\n",
    "    \n",
    "    # Separate weights and bias\n",
    "    w = params[:-1]\n",
    "    b = params[-1]\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "# Root mean squared error\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "# Compute prediction vector\n",
    "def predict_linreg(X, w, b):\n",
    "    return np.dot(X, w) + b\n",
    "\n",
    "# Fit the linear regression model\n",
    "w, b = fit_linreg(X_train, y_train, 30)\n",
    "\n",
    "# Fit with gradient-based optimizer \n",
    "ww, bb = fit_linreg_gradopt(X_train, y_train, 30)\n",
    "\n",
    "# Compute predictions on validation set. \n",
    "y_pred = predict_linreg(X_val, w, b)\n",
    "y_pred_opt = predict_linreg(X_val, ww, bb)\n",
    "\n",
    "# Compute RMSE on validation set for both\n",
    "rmse_val = rmse(y_val, y_pred)\n",
    "rmse_val_opt = rmse(y_val, y_pred_opt)\n",
    "\n",
    "# Compute RMSE on training set for both\n",
    "rmse_train = rmse(y_train, predict_linreg(X_train, w, b))\n",
    "rmse_train_opt = rmse(y_train, predict_linreg(X_train, ww, bb))\n",
    "\n",
    "# Compare RMSE values\n",
    "print(f\"RMSE on training set (closed-form): {rmse_train}\")\n",
    "print(f\"RMSE on training set (gradient-based): {rmse_train_opt}\")\n",
    "print(f\"RMSE on validation set (closed-form): {rmse_val}\")\n",
    "print(f\"RMSE on validation set (gradient-based): {rmse_val_opt}\")\n",
    "\n",
    "# See if the parameters are roughly the same\n",
    "print(f\"Weight difference: {np.linalg.norm(w - ww)}\")\n",
    "print(f\"Bias difference: {np.abs(b - bb)}\")\n",
    "\n",
    "# Are they approximately the same? 1e-2 is a reasonable tolerance\n",
    "print(f\"Are the weights the same? {np.allclose(w, ww, atol=1e-2)}\")\n",
    "print(f\"Are the biases the same? {np.allclose(b, bb, atol=1e-2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Invented Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on training set: 0.15441142592866933\n",
      "RMSE on validation set: 0.25424916988852914\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ct_support_code import *\n",
    "\n",
    "# Root mean squared error\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "# Compute prediction vector\n",
    "def predict_linreg(X, w, b):\n",
    "    return np.dot(X, w) + b\n",
    "\n",
    "K = 20 # number of thresholded classification problems to fit\n",
    "mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "\n",
    "# Initialize the transformed feature matrices\n",
    "prob_train = np.zeros((X_train.shape[0], K))\n",
    "prob_val = np.zeros((X_val.shape[0], K))\n",
    "\n",
    "for kk in range(K):\n",
    "    # Define binary labels based on threshold\n",
    "    labels = (y_train > thresholds[kk])\n",
    "    \n",
    "    # Fit logistic regression model using gradient optimizer\n",
    "    ww, bb = minimize_list(logreg_cost, (np.zeros(X_train.shape[1]), 0), (X_train, labels, 30))\n",
    "    \n",
    "    # Predict probabilities on training and validation sets\n",
    "    prob_train[:, kk] = 1 / (1 + np.exp(-(np.dot(X_train, ww) + bb)))\n",
    "    prob_val[:, kk] = 1 / (1 + np.exp(-(np.dot(X_val, ww) + bb)))\n",
    "\n",
    "# Fit regularized linear regression on the 20-dimensional probability features\n",
    "w, b = fit_linreg_gradopt(prob_train, y_train, 30)\n",
    "\n",
    "# Compute predictions and RMSE on training and validation sets\n",
    "train_preds = predict_linreg(prob_train, w, b)\n",
    "val_preds = predict_linreg(prob_val, w, b)\n",
    "\n",
    "rmse_train = rmse(y_train, train_preds)\n",
    "rmse_val = rmse(y_val, val_preds)\n",
    "\n",
    "print(f\"RMSE on training set: {rmse_train}\")\n",
    "print(f\"RMSE on validation set: {rmse_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Small Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on training set (random init): 0.13839872700253528\n",
      "RMSE on validation set (random init): 0.2688816261838681\n",
      "RMSE on training set (Q3 init): 0.13950899960871216\n",
      "RMSE on validation set (Q3 init): 0.26875475907458385\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ct_support_code import *\n",
    "\n",
    "# # Root mean squared error\n",
    "# def rmse(y_true, y_pred):\n",
    "#     return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "# # Compute prediction vector\n",
    "# def predict_linreg(X, w, b):\n",
    "#     return np.dot(X, w) + b\n",
    "\n",
    "# K = 20 # number of thresholded classification problems to fit\n",
    "# mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "# thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "\n",
    "# # Initialize the transformed feature matrices\n",
    "# prob_train = np.zeros((X_train.shape[0], K))\n",
    "# prob_val = np.zeros((X_val.shape[0], K))\n",
    "\n",
    "# for kk in range(K):\n",
    "#     # Define binary labels based on threshold\n",
    "#     labels = (y_train > thresholds[kk])\n",
    "    \n",
    "#     # Fit logistic regression model using gradient optimizer\n",
    "#     ww, bb = minimize_list(logreg_cost, (np.zeros(X_train.shape[1]), 0), (X_train, labels, 30))\n",
    "    \n",
    "#     # Predict probabilities on training and validation sets\n",
    "#     prob_train[:, kk] = 1 / (1 + np.exp(-(np.dot(X_train, ww) + bb)))\n",
    "#     prob_val[:, kk] = 1 / (1 + np.exp(-(np.dot(X_val, ww) + bb)))\n",
    "\n",
    "# # Fit regularized linear regression on the 20-dimensional probability features\n",
    "# w, b = fit_linreg_gradopt(prob_train, y_train, 30)\n",
    "\n",
    "# Random initialization of parameters\n",
    "np.random.seed(42)  # For reproducibility\n",
    "D = X_train.shape[1]  # Input dimensionality\n",
    "\n",
    "random_params = [\n",
    "    np.random.randn(K),  # ww: Output layer weights\n",
    "    np.random.randn(),   # bb: Output layer bias\n",
    "    np.random.randn(K, D),  # V: Hidden layer weights\n",
    "    np.random.randn(K)   # bk: Hidden layer biases\n",
    "]\n",
    "\n",
    "# Initialization using Q3 fits\n",
    "q3_params = [\n",
    "# You must run the code in Q3 to get these parameters.  \n",
    "    w,  # ww: Output layer weights from Q3\n",
    "    b,  # bb: Output layer bias from Q3\n",
    "    np.vstack([minimize_list(logreg_cost, (np.zeros(D), 0), (X_train, (y_train > t), 30))[0] for t in thresholds]),  # V: Hidden layer weights from Q3\n",
    "    np.array([minimize_list(logreg_cost, (np.zeros(D), 0), (X_train, (y_train > t), 30))[1] for t in thresholds])  # bk: Hidden layer biases from Q3\n",
    "]\n",
    "\n",
    "# Fit the neural network with both initializations\n",
    "alpha = 30  # Regularization constant\n",
    "\n",
    "# Fit with random initialization\n",
    "nn_random_params = minimize_list(nn_cost, random_params, (X_train, y_train, alpha))\n",
    "\n",
    "# Fit with Q3 initialization\n",
    "nn_q3_params = minimize_list(nn_cost, q3_params, (X_train, y_train, alpha))\n",
    "\n",
    "# Predict on training and validation sets\n",
    "nn_random_preds_train = nn_cost(nn_random_params, X_train)\n",
    "nn_random_preds_val = nn_cost(nn_random_params, X_val)\n",
    "\n",
    "nn_q3_preds_train = nn_cost(nn_q3_params, X_train)\n",
    "nn_q3_preds_val = nn_cost(nn_q3_params, X_val)\n",
    "\n",
    "# Compute RMSE for both initializations\n",
    "rmse_random_train = rmse(y_train, nn_random_preds_train)\n",
    "rmse_random_val = rmse(y_val, nn_random_preds_val)\n",
    "\n",
    "rmse_q3_train = rmse(y_train, nn_q3_preds_train)\n",
    "rmse_q3_val = rmse(y_val, nn_q3_preds_val)\n",
    "\n",
    "# Print RMSE values\n",
    "print(f\"RMSE on training set (random init): {rmse_random_train}\")\n",
    "print(f\"RMSE on validation set (random init): {rmse_random_val}\")\n",
    "\n",
    "print(f\"RMSE on training set (Q3 init): {rmse_q3_train}\")\n",
    "print(f\"RMSE on validation set (Q3 init): {rmse_q3_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5, Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from ct_support_code import *\n",
    "\n",
    "K = 20 # number of thresholded classification problems to fit\n",
    "mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "D = X_train.shape[1]  # Input dimensionality\n",
    "\n",
    "# Initialization using Q3 fits\n",
    "q3_params = [\n",
    "    w,  # ww: Output layer weights from Q3\n",
    "    b,  # bb: Output layer bias from Q3\n",
    "    np.vstack([minimize_list(logreg_cost, (np.zeros(D), 0), (X_train, (y_train > t), 30))[0] for t in thresholds]),  # V: Hidden layer weights from Q3\n",
    "    np.array([minimize_list(logreg_cost, (np.zeros(D), 0), (X_train, (y_train > t), 30))[1] for t in thresholds])  # bk: Hidden layer biases from Q3\n",
    "]\n",
    "\n",
    "# Root mean squared error\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "# Neural network training function\n",
    "def train_nn_reg(alpha, X_eval, y_eval):\n",
    "    nn_params = minimize_list(nn_cost, q3_params, (X_train, y_train, alpha))\n",
    "    eval_preds = nn_cost(nn_params, X_eval)  # Ensure nn_cost returns predictions\n",
    "    return rmse(y_eval, eval_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network 1...\n",
      "Training network 2...\n",
      "Training network 3...\n",
      "Training network 4...\n",
      "Training network 5...\n",
      "Baseline Model Validation RMSE: 0.2428\n",
      "Baseline Model Test RMSE: 0.2782\n",
      "\n",
      "Ensemble Validation RMSE: 0.2370\n",
      "Ensemble Test RMSE: 0.2658\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ct_support_code import *\n",
    "\n",
    "# Root mean squared error function\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "# Number of networks in the ensemble\n",
    "num_networks = 5\n",
    "best_alpha = 3.64  # Best alpha from Bayesian optimization\n",
    "K = 20             # Number of hidden units\n",
    "D = X_train.shape[1]  # Input dimensionality\n",
    "\n",
    "# Initialize parameters using Q3 fits\n",
    "q3_params = [\n",
    "    w,  # Output layer weights from Q3\n",
    "    b,  # Output layer bias from Q3\n",
    "    np.vstack([\n",
    "        minimize_list(logreg_cost, (np.zeros(D), 0), (X_train, (y_train > t), 30))[0]\n",
    "        for t in thresholds\n",
    "    ]),  # Hidden layer weights from Q3\n",
    "    np.array([\n",
    "        minimize_list(logreg_cost, (np.zeros(D), 0), (X_train, (y_train > t), 30))[1]\n",
    "        for t in thresholds\n",
    "    ])   # Hidden layer biases from Q3\n",
    "]\n",
    "\n",
    "# Lists to store predictions\n",
    "ensemble_preds_train = []\n",
    "ensemble_preds_val = []\n",
    "ensemble_preds_test = []\n",
    "\n",
    "# Training multiple neural networks\n",
    "for i in range(num_networks):\n",
    "    # Different random seed for each network to introduce diversity\n",
    "    np.random.seed(i)\n",
    "    print(f\"Training network {i+1}...\")\n",
    "    # Slight random perturbation to Q3 parameters for initialization\n",
    "    init_params = [\n",
    "        q3_params[0] + np.random.randn(K) * 0.05,      # ww: Output layer weights\n",
    "        q3_params[1] + np.random.randn() * 0.05,       # bb: Output layer bias\n",
    "        q3_params[2] + np.random.randn(K, D) * 0.05,   # V: Hidden layer weights\n",
    "        q3_params[3] + np.random.randn(K) * 0.05       # bk: Hidden layer biases\n",
    "    ]\n",
    "    \n",
    "    # Train neural network\n",
    "    nn_params = minimize_list(nn_cost, init_params, (X_train, y_train, best_alpha))\n",
    "    \n",
    "    # Predict on training, validation, and test sets\n",
    "    preds_train = nn_cost(nn_params, X_train)\n",
    "    preds_val = nn_cost(nn_params, X_val)\n",
    "    preds_test = nn_cost(nn_params, X_test)\n",
    "    \n",
    "    # Store predictions\n",
    "    ensemble_preds_train.append(preds_train)\n",
    "    ensemble_preds_val.append(preds_val)\n",
    "    ensemble_preds_test.append(preds_test)\n",
    "\n",
    "# Averaging predictions across the ensemble\n",
    "ensemble_preds_train = np.mean(ensemble_preds_train, axis=0)\n",
    "ensemble_preds_val = np.mean(ensemble_preds_val, axis=0)\n",
    "ensemble_preds_test = np.mean(ensemble_preds_test, axis=0)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_train = rmse(y_train, ensemble_preds_train)\n",
    "rmse_val = rmse(y_val, ensemble_preds_val)\n",
    "rmse_test = rmse(y_test, ensemble_preds_test)\n",
    "\n",
    "print(f\"Baseline Model Validation RMSE: {0.2428:.4f}\") # Precomputed in Q5.\n",
    "print(f\"Baseline Model Test RMSE: {0.2782:.4f}\\n\") # Precomputed in Q5.\n",
    "\n",
    "print(f\"Ensemble Validation RMSE: {rmse_val:.4f}\")\n",
    "print(f\"Ensemble Test RMSE: {rmse_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network 1...\n",
      "Training network 2...\n",
      "Training network 3...\n",
      "Training network 4...\n",
      "Training network 5...\n",
      "Baseline Model Validation RMSE: 0.2428\n",
      "Baseline Model Test RMSE: 0.2782\n",
      "\n",
      "Ensemble Validation RMSE: 0.2632\n",
      "Ensemble Test RMSE: 0.2949\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ct_support_code import *\n",
    "\n",
    "# Root mean squared error function\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "# Number of networks in the ensemble\n",
    "num_networks = 5\n",
    "best_alpha = 6.64  # Best alpha from Bayesian optimization\n",
    "K = 20             # Number of hidden units\n",
    "D = X_train.shape[1]  # Input dimensionality\n",
    "\n",
    "# Initialize parameters using Q3 fits\n",
    "q3_params = [\n",
    "    w,  # Output layer weights from Q3\n",
    "    b,  # Output layer bias from Q3\n",
    "    np.vstack([\n",
    "        minimize_list(logreg_cost, (np.zeros(D), 0), (X_train, (y_train > t), 30))[0]\n",
    "        for t in thresholds\n",
    "    ]),  # Hidden layer weights from Q3\n",
    "    np.array([\n",
    "        minimize_list(logreg_cost, (np.zeros(D), 0), (X_train, (y_train > t), 30))[1]\n",
    "        for t in thresholds\n",
    "    ])   # Hidden layer biases from Q3\n",
    "]\n",
    "\n",
    "# Lists to store predictions\n",
    "ensemble_preds_val = []\n",
    "ensemble_preds_test = []\n",
    "\n",
    "# Training multiple neural networks with bootstrapped datasets\n",
    "for i in range(num_networks):\n",
    "    # Bootstrapping: Resample the training data with replacement\n",
    "    np.random.seed(i)\n",
    "    indices = np.random.choice(len(X_train), len(X_train), replace=True)\n",
    "    X_bootstrap = X_train[indices]\n",
    "    y_bootstrap = y_train[indices]\n",
    "\n",
    "    print(f\"Training network {i+1} with bootstrapped data...\")\n",
    "    # Slight random perturbation to Q3 parameters for initialization\n",
    "    init_params = [\n",
    "        q3_params[0] + np.random.randn(K) * 0.05,      # ww: Output layer weights\n",
    "        q3_params[1] + np.random.randn() * 0.05,       # bb: Output layer bias\n",
    "        q3_params[2] + np.random.randn(K, D) * 0.05,   # V: Hidden layer weights\n",
    "        q3_params[3] + np.random.randn(K) * 0.05       # bk: Hidden layer biases\n",
    "    ]\n",
    "    \n",
    "    # Train neural network on the bootstrapped dataset\n",
    "    nn_params = minimize_list(nn_cost, init_params, (X_bootstrap, y_bootstrap, best_alpha))\n",
    "    \n",
    "    # Predict on validation and test sets\n",
    "    preds_val = nn_cost(nn_params, X_val)\n",
    "    preds_test = nn_cost(nn_params, X_test)\n",
    "    \n",
    "    # Store predictions\n",
    "    ensemble_preds_val.append(preds_val)\n",
    "    ensemble_preds_test.append(preds_test)\n",
    "\n",
    "# Averaging predictions across the ensemble\n",
    "ensemble_preds_val = np.mean(ensemble_preds_val, axis=0)\n",
    "ensemble_preds_test = np.mean(ensemble_preds_test, axis=0)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_val = rmse(y_val, ensemble_preds_val)\n",
    "rmse_test = rmse(y_test, ensemble_preds_test)\n",
    "\n",
    "print(f\"Baseline Model Validation RMSE: {0.2428:.4f}\") # Precomputed in Q5.\n",
    "print(f\"Baseline Model Test RMSE: {0.2782:.4f}\\n\") # Precomputed in Q5.\n",
    "\n",
    "print(f\"Ensemble Validation RMSE (bootstrapping): {rmse_val:.4f}\")\n",
    "print(f\"Ensemble Test RMSE (bootstrapping): {rmse_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network 1 with bootstrapped data...\n",
      "Training network 2 with bootstrapped data...\n",
      "Training network 3 with bootstrapped data...\n",
      "Training network 4 with bootstrapped data...\n",
      "Training network 5 with bootstrapped data...\n",
      "Baseline Model Validation RMSE: 0.2380\n",
      "Baseline Model Test RMSE: 0.2858\n",
      "\n",
      "Ensemble Validation RMSE (with bootstrapping): 0.2478\n",
      "Ensemble Test RMSE (with bootstrapping): 0.2766\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ct_support_code import *\n",
    "\n",
    "# Root mean squared error function\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "# Number of networks in the ensemble\n",
    "num_networks = 5\n",
    "best_alpha = 6.64  # Best alpha from Bayesian optimization\n",
    "K = 20             # Number of hidden units\n",
    "D = X_train.shape[1]  # Input dimensionality\n",
    "\n",
    "# Initialize parameters using Q3 fits\n",
    "q3_params = [\n",
    "    # READ: You must run the code in Q3 to get these parameters w and b!\n",
    "    w,  # Output layer weights from Q3\n",
    "    b,  # Output layer bias from Q3\n",
    "    np.vstack([\n",
    "        minimize_list(logreg_cost, (np.zeros(D), 0), (X_train, (y_train > t), 30))[0]\n",
    "        for t in thresholds\n",
    "    ]),  # Hidden layer weights from Q3\n",
    "    np.array([\n",
    "        minimize_list(logreg_cost, (np.zeros(D), 0), (X_train, (y_train > t), 30))[1]\n",
    "        for t in thresholds\n",
    "    ])   # Hidden layer biases from Q3\n",
    "]\n",
    "\n",
    "# Lists to store predictions\n",
    "ensemble_preds_val = []\n",
    "ensemble_preds_test = []\n",
    "\n",
    "# Training multiple neural networks with bootstrapped datasets\n",
    "for i in range(num_networks):\n",
    "    # Bootstrapping: Resample the training data with replacement\n",
    "    np.random.seed(420+i)\n",
    "    indices = np.random.choice(len(X_train), len(X_train), replace=True)\n",
    "    X_bootstrap = X_train[indices]\n",
    "    y_bootstrap = y_train[indices]\n",
    "\n",
    "    print(f\"Training network {i+1} with bootstrapped data...\")\n",
    "    # Slight random perturbation to Q3 parameters for initialization\n",
    "    init_params = [\n",
    "        q3_params[0] + np.random.randn(K) * 0.05,      # ww: Output layer weights\n",
    "        q3_params[1] + np.random.randn() * 0.05,       # bb: Output layer bias\n",
    "        q3_params[2] + np.random.randn(K, D) * 0.05,   # V: Hidden layer weights\n",
    "        q3_params[3] + np.random.randn(K) * 0.05       # bk: Hidden layer biases\n",
    "    ]\n",
    "    \n",
    "    # Train neural network on the bootstrapped dataset\n",
    "    nn_params = minimize_list(nn_cost, init_params, (X_bootstrap, y_bootstrap, best_alpha))\n",
    "    \n",
    "    # Predict on validation and test sets\n",
    "    preds_val = nn_cost(nn_params, X_val)\n",
    "    preds_test = nn_cost(nn_params, X_test)\n",
    "    \n",
    "    # Store predictions\n",
    "    ensemble_preds_val.append(preds_val)\n",
    "    ensemble_preds_test.append(preds_test)\n",
    "\n",
    "# Averaging predictions across the ensemble\n",
    "ensemble_preds_val = np.mean(ensemble_preds_val, axis=0)\n",
    "ensemble_preds_test = np.mean(ensemble_preds_test, axis=0)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_val = rmse(y_val, ensemble_preds_val)\n",
    "rmse_test = rmse(y_test, ensemble_preds_test)\n",
    "\n",
    "print(f\"Baseline Model Validation RMSE: {0.2380:.4f}\") # Precomputed in Q5.\n",
    "print(f\"Baseline Model Test RMSE: {0.2858:.4f}\\n\") # Precomputed in Q5.\n",
    "\n",
    "print(f\"Ensemble Validation RMSE (with bootstrapping): {rmse_val:.4f}\")\n",
    "print(f\"Ensemble Test RMSE (with bootstrapping): {rmse_test:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
